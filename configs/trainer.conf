###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  max_gradient_norm = 1.0
  learning_rate = 0.0001 # 1e-4
  decay_rate_per_epoch = 0.75
}
sgd {
  optimizer_type = GradientDescentOptimizer
  max_gradient_norm = 1.0
  learning_rate = 0.1
  decay_rate_per_epoch = 0.95
}


###################################
#         Trainer
###################################

trainer_base {
  #trainer_type = 
  optimizer = ${adam}
  max_to_keep = 1
  max_epoch = 60
  dropout_rate = 0.2   # keep_prob = 1.0 - dropout_rate
  lexical_dropout_rate = 0.0
}

gradient_sum = ${trainer_base}{
  trainer_type = GradientSum
}

multi_gpu = ${trainer_base}{
  trainer_type = MultiGPUTrainer
}

mtl_on_multi_gpu = ${trainer_base}{
  trainer_type = MTLonMultiGPU
}

