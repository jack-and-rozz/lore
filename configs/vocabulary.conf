###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"
word2vec_en_300d {
  #path = ${embeddings_dir}/...
  #size = 300
  #skip_first = true
}


#################################
#    Vocabulary
#################################
pad_token = </s>
unk_token = <unk>
bos_token = <bos>
vocab {
  encoder = {
    word = {
      vocab_class=VocabularyWithEmbedding
      vocab_size = 15000    # Number of tokens loaded per file.
      trainable = true
      use_pretrained_emb = true
      lowercase = true
      normalize_digits = true
      centralize_embedding = true
      normalize_embedding = true
      split_quotation = false 
      use_nltk_tokenizer = false

      pad_token = ${pad_token}
      unk_token = ${unk_token}
      bos_token = ${bos_token}
    }
    # char = {
    #   vocab_class = PredefinedCharVocab
    #   vocab_size = 5000    # Number of tokens loaded per file.
    #   embedding_size = 8
    #   split_quotation = ${vocab.encoder.word.split_quotation}
    #   use_nltk_tokenizer = ${vocab.encoder.word.use_nltk_tokenizer}

    #   pad_token = ${pad_token}
    #   unk_token = ${unk_token}
    # }
  }
}


vocab_en = ${vocab}{
  encoder = {
    word = {
      emb_config = ${word2vec_en_300d}
    }
    # char = {
    #   vocab_path = ${embeddings_dir}/char_vocab.en.txt
    # }
  }
}
